---
title: 提升方法AdaBoost GBDT
date: 2018-04-16 23:31:00
tags: boost
categories: 机器学习
toc: true
mathjax: true
---


对于一个复杂的任务，将多个专家的判断进行适当的综合所得出的判断，要比其中任何一个专家单独的判断好。体现”三个臭皮匠顶个诸葛亮“的道理。
<!--more-->

## 一些概念

> 概率近似正确（probably approximately correct，PAC）
> 强可学习（strongly learnable）：在PAC学习框架中，一个概念如果存在一个多项式的学习算法能够学习它且正确率很高，则称这个概念是强可学习的。
> 弱可学习（weekly learnable）：一个概念如果存在一个多项式的学习算法能偶学习他且学习的正确率仅比随机猜测略好，则称这个概念是弱可学习的。

## 引子

提升方法是从弱学习算法出发，反复学习，得到一系列弱分类器，然后组合这些弱分类器，构成一个强分类器，大多数提升方法都是改变训练数据的概率分布。则对于提升方法，有两个问题需要回答：

1. 在每一轮如何改变训练数据的权值或者概率分布？
2. 如何将弱分类器组合成一个强分类器？

## AdaBoost算法

### 输入输出

  输入：弱学习算法；训练数据集：

  $$
  T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}, x_i\in\chi\subseteq R^n, y_i\in\{-1,+1\}
  $$

  输出：最终分类器：G(x)  

  $$lim_{1\to+\infty}P(|\frac{1}{n}\sum_i^nX_i-\mu|<\epsilon)=1, i=1,...,n$$                                                   

### 算法步骤

**算法流程图如下**：

![提升方法流程图](/images/posts/2018.4.16/提升方法流程图.jpg)

**具体实现步骤**：

1. 初始化训练数据权值分布：

  $$
  D_1 = (w_{11},...,w_{1i},...,w_{1N}), w_{1i}=1/N, i=1,2,...,N
  $$

2. 对于m = 1,2,...,M，执行如下步骤：

  * 2.1 使用具有权值分布Dm的训练数据学习，得到基本分类器
      $$
      G_m(X)=\chi\to\{-1,+1\}
      $$

  * 2.2 计算如上分类器在训练数据集上的分类误差率
      $$
      e_m = P(G_m(x_i)\ne y_i) = \sum_{i=1}^{N}w_{mi}I(G_m(x_i)\ne y_i)
      $$

  * 2.3 计算如上分类器的系数alpha，该系数表示该分类器在最终分类器中的重要性。系数随着误差率的减小而增大，故分类误差率越小的基本分类器在最终分类器中的作用最大。
      $$
      \alpha_m = \frac{1}{2}log\frac{1-e_m}{e_m}
      $$
      ​

  * 2.4 更新训练数据集的权值分布
      $$
      D_{m+1}=(w_{m+1,1},....,w_{m+1,i},...,w_{m+1,N})
      $$
      如果样本被正确分类，则样本权值更改为：
      $$
      w_{m+1,i}=\frac{w_{mi}} {Z_m}exp(-\alpha_m),i=1,2,...,N
      $$
      如果样本被错分：
      $$
      w_{m+1,i}=\frac{w_{mi}} {Z_m}exp(\alpha_m),i=1,2,...,N
      $$
      即综合表示如下，其中y和G的乘积符号即可指明符号：
      $$
      w_{m+1,i}=\frac{w_{mi}} {Z_m}exp(-\alpha_my_iG_m(x_i)),i=1,2,...,N
      $$
      其中：
      $$
      Z_m=\sum_{i=1}^{N}w_{mi}exp(-\alpha_my_iG_m(x_i)),i=1,2,...,N
      $$

3. 构建基本分类器的线性组合，得到最终的分类器G(x)
   $$
   f(x)=\sum_{m=1}^{M}\alpha_mG_m(x)
   $$

   $$
   G(x)=sign(f(x))=sign(\sum_{m=1}^{M}\alpha_mG_m(x))
   $$

### 实现代码

   ```python
   """基于单层决策树（Decision Stump）的AdaBoosting训练过程
      输入参数：numIt是迭代次数，需要用户指定
   """
   def adaBoostTrainDS(dataArr, classLabels, numIt=40):
       # 弱分类器
       weakClassArr = []
       m = np.shape(dataArr)[0]
       # 步骤1：D是概率分布向量，初始化均分
       D = np.mat(np.ones((m, 1)) / m)
       aggClassEst = np.mat(np.zeros((m, 1)))
       for i in range(numIt):
           # 步骤2：
           # 2.1 使用具有权值分布D的训练数据集学习，得到基本分类器G(x)（单层决策树）
           # 2.2 error是G(x)在训练数据集上的分类误差率
           bestStump, error, classEst = buildStump(dataArr, classLabels, D)
           # 2.3 计算G(X)的系数alpha值，1e-16防止分母error为0
           alpha = float(0.5 * np.log((1.0 - error) / max(error, 1e-16)))
           bestStump['alpha'] = alpha
           # 保存迭代的最好的分类信息
           weakClassArr.append(bestStump)
           # print ("classEst: ",classEst.T)
           # 2.4 计算权值分布D的exp指数，计算新权值分布D，其中D.sum()即相当于规范化因子
           expon = np.multiply(-1 * alpha * np.mat(classLabels).T, classEst)
           D = np.multiply(D, np.exp(expon))
           D = D / D.sum()
           # 步骤3：构建基本分类器的线性组合：对分类结果作用alpha系数（权重）
           aggClassEst += alpha * classEst
           # print ("aggClassEst: ",aggClassEst.T)
           # 计算分类的错误率。numpy的sign(x)函数：x大于0的返回1,小于0的返回-1,等于0的返回0
           aggErrors = np.multiply(np.sign(aggClassEst) != np.mat(classLabels).T, np.ones((m, 1)))
           errorRate = aggErrors.sum() / m
           print("total error: ", errorRate)
           # 如果错误率为0，则退出循环
           if errorRate == 0.0: break
       return weakClassArr, aggClassEst
   ```

   ```python
   """ AdaBoosting分类函数
       输入：待分类的数据，基本分类器的线性组合结果：classifierArr
       输出：分类结果，其中aggClassEst绝对值表示分类的确信度，而符号决定了分类
   """
   def adaClassify(datToClass, classifierArr):
       dataMatrix = np.mat(datToClass)
       m = np.shape(dataMatrix)[0]
       aggClassEst = np.mat(np.zeros((m, 1)))
       for i in range(len(classifierArr)):
           classEst = stumpClassify(dataMatrix, classifierArr[i]['dim'],
                                    classifierArr[i]['thresh'],
                                    classifierArr[i]['ineq'])
           # 分类结果将随着迭代越来越强
           aggClassEst += classifierArr[i]['alpha'] * classEst
           print(aggClassEst)
       return np.sign(aggClassEst)
   ```

   其中弱分类器定义及实现如下：

   ```python
   """对数据进行分类，默认都是1（即一个分类结果）
      对于lt类型，将小于等于threshVal的置为-1，对于gt类型，将大于threshVal的置为-1。
      为什么区分lt和gt是对分类结果的两种情况进行尝试，因为而分类的yes or no在左边还是右边未知
   """
   def stumpClassify(dataMatrix, dimen, threshVal, threshIneq):  # just classify the data
       retArray = np.ones((np.shape(dataMatrix)[0], 1))
       if threshIneq == 'lt':
           retArray[dataMatrix[:, dimen] <= threshVal] = -1.0
       else:
           retArray[dataMatrix[:, dimen] > threshVal] = -1.0
       return retArray


   """ 使用具有权值分布D的训练数据集学习，得到基本分类器
       输入参数：数据集（训练数据），分类列表（训练数据类别），权重向量
       输出：分类结果（第几个特征，阈值，大于还是小于），误差，预测分类
   """
   def buildStump(dataArr, classLabels, D):
       # 获取data和label的矩阵。其中label转置为纵向
       dataMatrix = np.mat(dataArr);
       labelMat = np.mat(classLabels).T
       m, n = np.shape(dataMatrix)
       # bestClasEst为m行1列的0数据
       numSteps = 10.0;
       bestStump = {};
       bestClasEst = np.mat(np.zeros((m, 1)))
       # 最小误差，默认为无穷大
       minError = np.inf
       # 遍历所有特征维度
       for i in range(n):
           # 得到该特征的最大和最小值
           rangeMin = dataMatrix[:, i].min();
           rangeMax = dataMatrix[:, i].max();
           # 存在步长的概念
           stepSize = (rangeMax - rangeMin) / numSteps
           for j in range(-1, int(numSteps) + 1):
               for inequal in ['lt', 'gt']:  # go over less than and greater than
                   threshVal = (rangeMin + float(j) * stepSize)
                   predictedVals = stumpClassify(dataMatrix, i, threshVal,
                                                 inequal)  
                   errArr = np.mat(np.ones((m, 1)))
                   errArr[predictedVals == labelMat] = 0
                   weightedError = D.T * errArr  # calc total error multiplied by D

                   if weightedError < minError:
                       minError = weightedError
                       bestClasEst = predictedVals.copy()
                       bestStump['dim'] = i
                       bestStump['thresh'] = threshVal
                       bestStump['ineq'] = inequal
       return bestStump, minError, bestClasEst
   ```

## 脑图总结

![提升方法](/images/posts/2018.4.16/提升方法.jpg)
