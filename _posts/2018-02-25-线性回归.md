---
title: 线性回归
date: 2018-02-25 17:33:00
tags: 回归
categories: 机器学习
toc: true
---


用回归来预测数值型数据。
<!--more-->

### 概述
回归的目的：预测数值型的目标值。
途径：构建回归方程。
关键：求解回归系数。
定义：求回归系数的过程就是回归。
回归的分类：线性回归、非线性回归。

### 过程
1. 训练算法：找到回归系数w。
    * 方法：计算预测值和结果的平方误差，对回归系数w求导，找出使误差最小的w值，即w的最佳估计。
2. 测试算法：使用R^2或预测值和数据的拟合度，来分析模型效果。
    * 利用相关系数来计算预测值和真实值序列的匹配程度：np.corrcoef。

### 标准线性回归

w的最佳估计: $w = (X^T X)^{-1}X^T y$
(hexo支持latex表达式设置参考：http://blog.csdn.net/u014792304/article/details/78687859)
推导过程：
![png](/images/posts/2018.2.25/process.jpg)



```python
from numpy import *

""" 载入数据集，dataMat保存特征，labelMat保存目标值
    数据集格式示例：
        1.000000	0.067732	3.176513
        1.000000	0.427810	3.816464
        1.000000	0.995731	4.550095
"""
def loadDataSet(fileName):      
    numFeat = len(open(fileName).readline().split('\t')) - 1
    dataMat = []; labelMat = []
    fr = open(fileName)
    for line in fr.readlines():
        lineArr =[]
        curLine = line.strip().split('\t')
        for i in range(numFeat):
            lineArr.append(float(curLine[i]))
        dataMat.append(lineArr)
        labelMat.append(float(curLine[-1]))
    return dataMat,labelMat

""" 求解线性回归的回归系数ws
"""
def standRegres(xArr,yArr):
    xMat = mat(xArr); yMat = mat(yArr).T
    # T是转置矩阵
    xTx = xMat.T*xMat
    # linalg.det计算矩阵的行列式，行列式不为0时矩阵的逆才存在
    # linalg是Numpy提供的线性代数库
    if linalg.det(xTx) == 0.0:
        print("This matrix is singular, cannot do inverse")
        return
    # I是逆矩阵
    ws = xTx.I * (xMat.T*yMat)
    return ws
```

求解ws：


```python
xArr,yArr = loadDataSet('./data/ex0.txt')
ws = standRegres(xArr, yArr)
ws
```




    matrix([[ 3.00774324],
            [ 1.69532264]])



计算预测值和目标值的相关系数：


```python
xMat = mat(xArr)
yHat = xMat*ws
yMat = mat(yArr)
corrcoef(yHat.T, yMat) # 两个向量都是行向量
```




    array([[ 1.        ,  0.98647356],
           [ 0.98647356,  1.        ]])



### 局部加权线性回归

标准线性回归求解w用的求解最小均方误差的无偏估计，在模型欠拟合时效果不好，因此可以在估计中引入一些偏差，以降低预测的均方误差。局部加权线性回归（LWLR）引入高斯核来对附近的点赋予更高的权重。
回归系数w可表示如下，

$w = (X^T WX)^{-1}X^T Wy$

其中w是矩阵，即对每个点赋予的权重。点$x^i$与x越近，w(i,i)越大。
需要手工调试的是k值大小。样本点与待预测点距离递增时，权重以指数级衰减，而k则控制衰减的速度。k越小，衰减速度越大。

$w(i,i) = exp\lgroup\dfrac{|x^i-x|}{-2k^2}\rgroup$


```python
""" 局部加权线性回归
"""
def lwlr(testPoint,xArr,yArr,k=1.0):
    xMat = mat(xArr); yMat = mat(yArr).T
    m = shape(xMat)[0]
    # weights为m*m对阵矩阵
    weights = mat(eye((m)))
    for j in range(m):                      #next 2 lines create weights matrix
        diffMat = testPoint - xMat[j,:]     #
        weights[j,j] = exp(diffMat*diffMat.T/(-2.0*k**2))
    xTx = xMat.T * (weights * xMat)
    if linalg.det(xTx) == 0.0:
        print("This matrix is singular, cannot do inverse")
        return
    ws = xTx.I * (xMat.T * (weights * yMat))
    return testPoint * ws

def lwlrTest(testArr,xArr,yArr,k=1.0):  #loops over all the data points and applies lwlr to each one
    m = shape(testArr)[0]
    yHat = zeros(m)
    for i in range(m):
        yHat[i] = lwlr(testArr[i],xArr,yArr,k)
    return yHat

```


```python
# 将k设置为0.003
yHat = lwlrTest(xArr, xArr, yArr, 0.003)

xMat = mat(xArr)
srtInd = xMat[:,1].argsort(0)
xSort = xMat[srtInd][:,0,:]

import matplotlib.pyplot as plt
fig = plt.figure()
ax = fig.add_subplot(111)
ax.plot(xSort[:,1],yHat[srtInd])
ax.scatter(xMat[:,1].flatten().A[0], mat(yArr).T.flatten().A[0], s=2, c='red')
plt.show()
```


![png](/images/posts/2018.2.25/output_9_0.png)


### 有的时候需要缩减系数
#### 岭回归

当特征比样本点还多时，输入矩阵不再是满秩矩阵，则标准回归函数求解会有问题，引入岭回归（ridge regression）的概念，是一种缩减系数的方法。
回归系数计算公式为：
$w = (X^T X + 	\lambda I)^{-1}X^T y$

当$\lambda$非常小时，系数与普通回归一样，非常大时，所有回归系数趋近于0。

#### 前向逐步回归
是一种贪心算法，每一步都尽可能减小误差。


```python
"""计算误差
"""
def rssError(yArr,yHatArr): #yArr and yHatArr both need to be arrays
    return ((yArr-yHatArr)**2).sum()

"""矩阵标准化
"""
def regularize(xMat):
    inMat = xMat.copy()
    # 按列标准化
    inMeans = mean(inMat,0)   #calc mean then subtract it off
    inVar = var(inMat,0)      #calc variance of Xi then divide by it
    inMat = (inMat - inMeans)/inVar
    return inMat

"""前向逐步回归
"""
def stageWise(xArr,yArr,eps=0.01,numIt=100):
    xMat = mat(xArr); yMat=mat(yArr).T
    yMean = mean(yMat,0)
    yMat = yMat - yMean     #can also regularize ys but will get smaller coef
    xMat = regularize(xMat)
    m,n=shape(xMat)
    #returnMat = zeros((numIt,n)) #testing code remove
    # ws是n行1列的数据，即n个值，对应着n个特征
    ws = zeros((n,1)); wsTest = ws.copy(); wsMax = ws.copy()
    for i in range(numIt):
        print(ws.T)
        # 设置当前最小误差为正无穷
        lowestError = inf;
        for j in range(n):
            # 特征增大或缩小
            for sign in [-1,1]:
                wsTest = ws.copy()
                # 改变系数得到新的W
                wsTest[j] += eps*sign
                yTest = xMat*wsTest
                rssE = rssError(yMat.A,yTest.A)
                if rssE < lowestError:
                    lowestError = rssE
                    wsMax = wsTest
        ws = wsMax.copy()
```


```python
abX,abY = loadDataSet('./data/abalone.txt')
stageWise(abX, abY, 0.01, 200)
```

    [[ 0.  0.  0.  0.  0.  0.  0.  0.]]
    [[ 0.    0.    0.    0.01  0.    0.    0.    0.  ]]
    [[ 0.    0.    0.    0.02  0.    0.    0.    0.  ]]
    ......
    [[ 0.05  0.    0.09  0.03  0.31 -0.64  0.    0.36]]
    [[ 0.04  0.    0.09  0.03  0.31 -0.64  0.    0.36]]
    [[ 0.05  0.    0.09  0.03  0.31 -0.64  0.    0.36]]
    [[ 0.04  0.    0.09  0.03  0.31 -0.64  0.    0.36]]


如上的缩减系数方法使模型增加了偏差，但同时减小了方差。缩减系数的方法可以找到哪些特征是对结果有高度影响的，哪些特征可能是不重要的。
偏差方差折中是一个重要概念，需要考虑训练误差和测试误差。

### 脑图总结
线性回归模型的思维导图总结如下：
![png](/images/posts/2018.2.25/lr.png)


### tips
1. 转置矩阵和原矩阵相乘。
2. 满秩矩阵的条件：当n > m时，矩阵不是满秩矩阵。
